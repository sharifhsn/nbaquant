{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA Player Data Analysis Project\n",
    "\n",
    "## Project Outline\n",
    "\n",
    "1. **Introduction**\n",
    "   - Overview of the project\n",
    "   - Goals and objectives\n",
    "\n",
    "2. **Data Collection**\n",
    "   - Import necessary libraries\n",
    "   - Fetch player data from the NBA API\n",
    "\n",
    "3. **Data Processing**\n",
    "   - Load data into a Pandas DataFrame\n",
    "   - Data cleaning and transformation\n",
    "\n",
    "4. **Data Analysis**\n",
    "   - Exploratory data analysis (EDA)\n",
    "   - Visualizations\n",
    "\n",
    "5. **Conclusion**\n",
    "   - Summary of findings\n",
    "   - Future work\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python\n",
    "\n",
    "This project is written in Python, which means that Python must be installed in your environment to run the project. The minimum supported version is 3.10.\n",
    "\n",
    "#### Windows\n",
    "\n",
    "You can use the Windows package manager `winget`, or the [installer](https://www.python.org/downloads/windows/) from the website.\n",
    "```powershell\n",
    "# you can change the version in the package name to your desired version\n",
    "winget install Python.Python.3.12\n",
    "```\n",
    "\n",
    "#### MacOS\n",
    "Python is already installed by default on recent versions of MacOS. If you have an older version that is not supported, you can use the [Homebrew](https://brew.sh/) package manager to install it, or the [installer](https://www.python.org/downloads/macos/) from the website.\n",
    "```zsh\n",
    "brew install python\n",
    "```\n",
    "\n",
    "#### Linux\n",
    "Python is already installed by default on most distributions of Linux. If it isn't, you can use your distribution's package manager to install Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Virtual Environment\n",
    "\n",
    "It's generally recommended that you use a virtual environment (or venv) for this project. That way, all dependencies can be installed for the project without affecting the rest of your system. You can create a venv with Python:\n",
    "\n",
    "```bash\n",
    "python -m venv .venv\n",
    "```\n",
    "\n",
    "To activate the virtual environment in your shell, you can use the following commands.\n",
    "\n",
    "On Windows:\n",
    "\n",
    "```powershell\n",
    ".venv\\Scripts\\activate\n",
    "```\n",
    "\n",
    "On other operating systems:\n",
    "\n",
    "```bash\n",
    ".venv/bin/activate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "\n",
    "This project uses [Poetry](https://python-poetry.org/) to manage its dependencies. You can install the dependencies with the `poetry` command:\n",
    "\n",
    "`poetry install`\n",
    "\n",
    "If you don't want to use Poetry, a `requirements.txt` is also provided. You can install this using `pip`:\n",
    "\n",
    "`pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Variables\n",
    "\n",
    "We will load all our environment variables from a `.env` file, if one is provided.\n",
    "\n",
    "If database information is provided, all dataframes used for analysis are uploaded to it. We use [Microsoft SQL Server](https://www.microsoft.com/en-us/sql-server/sql-server-downloads) by default but any kind of database is supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "DB_TYPE = os.getenv(\"DB_TYPE\", \"sqlserver\")\n",
    "DB_USER = os.getenv(\"DB_USER\", \"sqladmin\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "DB_HOST = os.getenv(\"DB_HOST\", \"localhost\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\", \"1433\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\", \"dataframes\")\n",
    "DB_DRIVER = os.getenv(\"DB_DRIVER\") # some databases require a database driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presentation\n",
    "\n",
    "By default, Pandas dataframes are truncated when they are printed. We want to be able to view all of the data at once, so we embed the dataframe in a scrollable element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "def custom_scrollable_display(df: pd.DataFrame, max_height=400):\n",
    "    \"\"\"\n",
    "    Custom display function to render DataFrames as scrollable elements.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: The DataFrame to display.\n",
    "    - max_height: The maximum height of the scrollable area in pixels.\n",
    "    \"\"\"\n",
    "    style = f\"\"\"\n",
    "    <style>\n",
    "    .scrollable-dataframe {{\n",
    "        display: inline-block;\n",
    "        white-space: nowrap;\n",
    "        overflow-x: scroll;\n",
    "        max-height: {max_height}px;\n",
    "        overflow-y: scroll;\n",
    "    }}\n",
    "    </style>\n",
    "    \"\"\"\n",
    "    display(HTML(style + f'<div class=\"scrollable-dataframe\">{df.to_html()}</div>'))\n",
    "\n",
    "def custom_display_hook(df):\n",
    "    custom_scrollable_display(df)\n",
    "    return \"\"\n",
    "\n",
    "# hook up the custom display function to the automatic printer\n",
    "InteractiveShell.instance().display_formatter.formatters['text/html'].for_type(pd.DataFrame, custom_display_hook);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Commit Hooks (Developer Only)\n",
    "\n",
    "This notebook uses `nbstripout` to strip notebook output from Git commits. If you are committing code, please run the following command to set up the Git filter.\n",
    "\n",
    "Poetry is required for the pre-commit hooks, so make sure it is installed before you commit code. You will also need to add the plugin `poetry-plugin-export` in order to run the export hook.\n",
    "```bash\n",
    "poetry self add poetry-plugin-export\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbstripout --install\n",
    "!pre-commit install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection\n",
    "\n",
    "### Fetch Player Data from NBA API\n",
    "\n",
    "`nba_api` provides static player and team information, which we will download here so that we can reuse it without requesting the API unnecessarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nba_api.stats.static import players, teams\n",
    "PLAYERS_LIST_FILE = \"../data/players_list.csv\"\n",
    "TEAMS_LIST_FILE = \"../data/teams_list.csv\"\n",
    "\n",
    "if os.path.exists(PLAYERS_LIST_FILE):\n",
    "    players_list = pd.read_csv(PLAYERS_LIST_FILE)\n",
    "else:\n",
    "    players_list = pd.DataFrame(players.get_players())\n",
    "    players_list.to_csv(PLAYERS_LIST_FILE)\n",
    "\n",
    "if os.path.exists(TEAMS_LIST_FILE):\n",
    "    teams_list = pd.read_csv(TEAMS_LIST_FILE)\n",
    "else:\n",
    "    teams_list = pd.DataFrame(teams.get_teams())\n",
    "    teams_list.to_csv(TEAMS_LIST_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch Game Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're only interested in games that are either in the regular season or in the playoffs. We'll add an enum to distinguish the type of game and use it to differentiate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class SeasonType(Enum):\n",
    "    PRESEASON = 1\n",
    "    REGULAR_SEASON = 2\n",
    "    ALL_STAR = 3\n",
    "    PLAYOFFS = 4\n",
    "    PLAY_IN = 5\n",
    "    NBA_CUP = 6\n",
    "class Season():\n",
    "    def __init__(self, season_id: int) -> None:\n",
    "        season_id_str = str(season_id)\n",
    "        self.season_type = SeasonType(int(season_id_str[0]))\n",
    "        self.season_year = int(season_id_str[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nba_api.stats.endpoints import leaguegamefinder\n",
    "from nba_api.stats.library.parameters import LeagueIDNullable, LeagueID\n",
    "START_SEASON = 2012\n",
    "END_SEASON = 2023\n",
    "GAMES_LIST_FILE = \"../data/games_list.csv\"\n",
    "if os.path.exists(GAMES_LIST_FILE):\n",
    "    games_list: pd.DataFrame = pd.read_csv(GAMES_LIST_FILE)\n",
    "else:\n",
    "    games_list = pd.DataFrame()\n",
    "    for season in range(START_SEASON, END_SEASON + 1):\n",
    "        # put season into the correct form e.g. 2023 -> 2023-24\n",
    "        season_str = f\"{season}-{str(season + 1)[2:]}\"\n",
    "        print(f\"Fetching games for season: {season_str}\", end=\"\\r\")\n",
    "        gamefinder = leaguegamefinder.LeagueGameFinder(season_nullable=season_str, league_id_nullable=LeagueIDNullable.nba)\n",
    "        games = gamefinder.get_data_frames()[0]\n",
    "        games_list = pd.concat([games_list, games], ignore_index=True)\n",
    "        time.sleep(0.6)\n",
    "    games_list.to_csv(GAMES_LIST_FILE, index=False)\n",
    "# games_list[\"SEASON_ID\"].unique()\n",
    "games_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch Play by Plays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nba_api.stats.endpoints import playbyplayv3\n",
    "from requests.exceptions import ReadTimeout\n",
    "# took 483 minutes to download up to 2012\n",
    "PBP_LIST_FILE = \"../data/pbp_list.csv\"\n",
    "if os.path.exists(PBP_LIST_FILE):\n",
    "    pbp_list = pd.read_csv(PBP_LIST_FILE)\n",
    "else:\n",
    "    unique_games_list = games_list.drop_duplicates(subset=\"GAME_ID\")\n",
    "    pbp_list = pd.DataFrame()\n",
    "    for index, row in unique_games_list.iterrows():\n",
    "        err = False\n",
    "        game_id = row[\"GAME_ID\"]\n",
    "        game_date = row[\"GAME_DATE\"]\n",
    "        season_id = row[\"SEASON_ID\"]\n",
    "        season = Season(season_id)\n",
    "        if season.season_type != SeasonType.REGULAR_SEASON and season.season_type != SeasonType.PLAYOFFS:\n",
    "            continue\n",
    "        print(f\"Fetching play by play for game {game_id} on {game_date}\", end=\"\\r\")\n",
    "        while True:\n",
    "            try:\n",
    "                pbpfinder = playbyplayv3.PlayByPlayV3(f\"{game_id:010}\")\n",
    "                break\n",
    "            except ReadTimeout as e:\n",
    "                print(f\"{e}! Try again\")\n",
    "            except Exception:\n",
    "                with open(\"../data/err.log\", \"a\") as f:\n",
    "                    print(f\"{game_id} does not have a play by play\", file=f)\n",
    "                err = True\n",
    "                break\n",
    "        if err:\n",
    "            continue\n",
    "        pbp = pbpfinder.get_data_frames()[0]\n",
    "        pbp_list = pd.concat([pbp_list, pbp], ignore_index=True)\n",
    "        time.sleep(0.6)\n",
    "    pbp_list.to_csv(PBP_LIST_FILE, index=False)\n",
    "pbp_list.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch Boxscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nba_api.stats.endpoints import boxscoretraditionalv3\n",
    "from nba_api.stats.library.parserv3 import NBAStatsBoxscoreTraditionalParserV3, NBAStatsPlayByPlayParserV3, NBAStatsBoxscoreParserV3\n",
    "\n",
    "boxscorefinder = boxscoretraditionalv3.BoxScoreTraditionalV3(\"0041200407\")\n",
    "boxscorefinder.get_data_frames()[0]\n",
    "# parser = NBAStatsBoxscoreTraditionalParserV3(boxscorefinder.get_dict())\n",
    "# parser.get_start_bench_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unnecessary Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of this data isn't useful to us, so we'll drop it to ignore the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\"actionNumber\", \"pointsTotal\", \"videoAvailable\", \"actionId\", \"playerName\", \"playerNameI\", \"personId\", \"teamTricode\"]\n",
    "pbp_list.drop(columns=[col for col in columns_to_drop if col in pbp_list.columns], inplace=True)\n",
    "pbp_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save on memory, we will also turn variables that can be understood as categorical variables into that type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\"gameId\", \"teamId\", \"shotResult\", \"isFieldGoal\", \"location\", \"actionType\", \"subType\"]\n",
    "pbp_list[categorical_columns] = pbp_list[categorical_columns].astype(\"category\")\n",
    "pbp_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll transform the clock data from a string into the total number of seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pbp_list[\"clock\"].dtype != \"int64\":\n",
    "    pbp_list[\"clock\"] = pbp_list[\"clock\"].astype(str)\n",
    "    pbp_list[\"minutes\"] = pbp_list[\"clock\"].str[2:4].astype(int)\n",
    "    pbp_list[\"seconds\"] = pbp_list[\"clock\"].str[5:7].astype(int)\n",
    "    pbp_list[\"clock\"] = pbp_list[\"minutes\"] * 60 + pbp_list[\"seconds\"]\n",
    "    pbp_list.drop(columns=[\"minutes\", \"seconds\"], inplace=True)\n",
    "pbp_list[\"clock\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Clutch\n",
    "\n",
    "The clutch is one of the most important time periods in an NBA game. Colloquially, it refers to the part of the game where actions taken have a much greater chance of affecting the outcome of the game. As defined by the NBA, this is in the last five minutes of the game when there is a score margin of five or less.\n",
    "\n",
    "Our analysis will cover how shot distance in the clutch is different from shot distance in general, and how shots taken at a particular distance affect the outcome of the game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our analysis will be focused on when shots with assists are being made, so we will isolate these actions for the relevant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substitutions = pbp_list.loc[pbp_list[\"actionType\"] == \"Substitution\"].reset_index(drop=True)\n",
    "substitutions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_shots = pbp_list.loc[pbp_list[\"shotResult\"].notna()].reset_index(drop=True)\n",
    "all_shots.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to need to calculate the score margin of each shot to determine whether they are in the clutch. However, the score margin is not defined for missed shots, only made shots. We will forward fill the res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_shots.loc[all_shots[\"shotResult\"] == \"Missed\", [\"scoreHome\", \"scoreAway\"]] = all_shots.loc[all_shots[\"shotResult\"] == \"Missed\", [\"scoreHome\", \"scoreAway\"]].replace(0, np.nan)\n",
    "all_shots[['scoreHome', 'scoreAway']] = all_shots.groupby('gameId', observed=True)[['scoreHome', 'scoreAway']].ffill()\n",
    "all_shots.loc[all_shots[\"shotResult\"] == \"Missed\", [\"scoreHome\", \"scoreAway\"]] = all_shots.loc[all_shots[\"shotResult\"] == \"Missed\", [\"scoreHome\", \"scoreAway\"]].replace(np.nan, 0)\n",
    "all_shots.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can separate these shots into those that are missed and made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "made_shots = all_shots.loc[all_shots[\"shotResult\"] == \"Made\"]\n",
    "missed_shots = all_shots.loc[all_shots[\"shotResult\"] == \"Missed\"]\n",
    "made_shots.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will further isolate a dataframe that contains only shots in the clutch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_clutch(df):\n",
    "    is_last_period = df[\"period\"] >= 4\n",
    "    is_last_five_minutes = df[\"clock\"] <= 300\n",
    "    is_score_margin_below_five = abs(df[\"scoreHome\"] - df[\"scoreAway\"]) <= 5\n",
    "    return is_last_period & is_last_five_minutes & is_score_margin_below_five\n",
    "\n",
    "clutch_shots = all_shots[is_clutch(all_shots)]\n",
    "clutch_made_shots = made_shots[is_clutch(made_shots)]\n",
    "clutch_missed_shots = missed_shots[is_clutch(missed_shots)]\n",
    "\n",
    "print(len(clutch_shots), len(clutch_made_shots), len(clutch_missed_shots))\n",
    "len(all_shots), len(made_shots), len(missed_shots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots: 1 row, 2 columns\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "bin_edges = np.histogram_bin_edges(\n",
    "    np.concatenate([\n",
    "        all_shots[\"shotDistance\"], made_shots[\"shotDistance\"], missed_shots[\"shotDistance\"],\n",
    "        clutch_shots[\"shotDistance\"], clutch_made_shots[\"shotDistance\"], clutch_missed_shots[\"shotDistance\"]\n",
    "    ]),\n",
    "    bins=30\n",
    ")\n",
    "\n",
    "# Histogram for general shots\n",
    "axes[0].hist(all_shots[\"shotDistance\"], bins=bin_edges, alpha=0.5, label='All Shots')\n",
    "axes[0].hist(made_shots[\"shotDistance\"], bins=bin_edges, alpha=0.5, label='Made Shots')\n",
    "axes[0].hist(missed_shots[\"shotDistance\"], bins=bin_edges, alpha=0.5, label='Missed Shots')\n",
    "axes[0].set_xlabel('Shot Distance')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Shot Distances (General Shots)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Histogram for clutch shots\n",
    "axes[1].hist(clutch_shots[\"shotDistance\"], bins=bin_edges, alpha=0.5, label='Clutch Shots')\n",
    "axes[1].hist(clutch_made_shots[\"shotDistance\"], bins=bin_edges, alpha=0.5, label='Clutch Made Shots')\n",
    "axes[1].hist(clutch_missed_shots[\"shotDistance\"], bins=bin_edges, alpha=0.5, label='Clutch Missed Shots')\n",
    "axes[1].set_xlabel('Shot Distance')\n",
    "axes[1].set_title('Distribution of Shot Distances (Clutch Shots)')\n",
    "axes[1].legend()\n",
    "\n",
    "# Adjust layout\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the combined range for KDE plots\n",
    "shot_distances = np.concatenate([\n",
    "    all_shots[\"shotDistance\"], made_shots[\"shotDistance\"], missed_shots[\"shotDistance\"],\n",
    "    clutch_shots[\"shotDistance\"], clutch_made_shots[\"shotDistance\"], clutch_missed_shots[\"shotDistance\"]\n",
    "])\n",
    "x_range = (shot_distances.min(), shot_distances.max())\n",
    "\n",
    "# Create subplots: 1 row, 2 columns\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# KDE plot for general shots\n",
    "sns.kdeplot(data=all_shots, x=\"shotDistance\", ax=axes[0], label='All Shots', fill=True)\n",
    "sns.kdeplot(data=made_shots, x=\"shotDistance\", ax=axes[0], label='Made Shots', fill=True)\n",
    "sns.kdeplot(data=missed_shots, x=\"shotDistance\", ax=axes[0], label='Missed Shots', fill=True)\n",
    "axes[0].set_xlim(x_range)\n",
    "axes[0].set_xlabel('Shot Distance')\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[0].set_title('KDE of Shot Distances (General Shots)')\n",
    "axes[0].legend()\n",
    "\n",
    "# KDE plot for clutch shots\n",
    "sns.kdeplot(data=clutch_shots, x=\"shotDistance\", ax=axes[1], label='Clutch Shots', fill=True)\n",
    "sns.kdeplot(data=clutch_made_shots, x=\"shotDistance\", ax=axes[1], label='Clutch Made Shots', fill=True)\n",
    "sns.kdeplot(data=clutch_missed_shots, x=\"shotDistance\", ax=axes[1], label='Clutch Missed Shots', fill=True)\n",
    "axes[1].set_xlim(x_range)\n",
    "axes[1].set_xlabel('Shot Distance')\n",
    "axes[1].set_title('KDE of Shot Distances (Clutch Shots)')\n",
    "axes[1].legend()\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots: 1 row, 2 columns\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Scatter plot for general shots with regression lines\n",
    "sns.regplot(data=all_shots, x=\"shotDistance\", y=all_shots.index, ax=axes[0], scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
    "sns.regplot(data=made_shots, x=\"shotDistance\", y=made_shots.index, ax=axes[0], scatter_kws={'alpha':0.5}, line_kws={'color':'blue'})\n",
    "sns.regplot(data=missed_shots, x=\"shotDistance\", y=missed_shots.index, ax=axes[0], scatter_kws={'alpha':0.5}, line_kws={'color':'green'})\n",
    "axes[0].set_xlabel('Shot Distance')\n",
    "axes[0].set_ylabel('Index')\n",
    "axes[0].set_title('Shot Distance vs. Index (General Shots)')\n",
    "axes[0].legend(['All Shots', 'Made Shots', 'Missed Shots'], loc=\"upper right\")\n",
    "\n",
    "# Scatter plot for clutch shots with regression lines\n",
    "sns.regplot(data=clutch_shots, x=\"shotDistance\", y=clutch_shots.index, ax=axes[1], scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
    "sns.regplot(data=clutch_made_shots, x=\"shotDistance\", y=clutch_made_shots.index, ax=axes[1], scatter_kws={'alpha':0.5}, line_kws={'color':'blue'})\n",
    "sns.regplot(data=clutch_missed_shots, x=\"shotDistance\", y=clutch_missed_shots.index, ax=axes[1], scatter_kws={'alpha':0.5}, line_kws={'color':'green'})\n",
    "axes[1].set_xlabel('Shot Distance')\n",
    "axes[1].set_title('Shot Distance vs. Index (Clutch Shots)')\n",
    "axes[1].legend(['Clutch Shots', 'Clutch Made Shots', 'Clutch Missed Shots'], loc=\"upper right\")\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Reporting\n",
    "\n",
    "To create our final report from our analysis, we will be using Power BI. We have a Microsoft SQL Server database that our Power BI report will import the tables from. If no database is available, the dataframes will instead export as an Excel spreadsheet, which can be manually uploaded to Power BI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas import ExcelWriter\n",
    "# import sqlalchemy\n",
    "\n",
    "# dfs = {\"assists_in_the_clutch\": df}\n",
    "# DB_PASSWORD = None\n",
    "\n",
    "# def upload_dataframes(dfs: Dict[str, pd.DataFrame]) -> None:\n",
    "#     inspector = sqlalchemy.inspect(engine)\n",
    "#     existing_tables = inspector.get_table_names()\n",
    "\n",
    "#     for table_name, df in dfs.items():\n",
    "#         # check if table already exists\n",
    "#         if table_name in existing_tables:\n",
    "#             # if there are no changes to the table, do not write to it\n",
    "#             existing_df = pd.read_sql_table(table_name, engine)\n",
    "#             if df.shape == existing_df.shape and df.equals(existing_df):\n",
    "#                 print(f\"No changes detected for table {table_name}. Skipping upload.\")\n",
    "#                 continue\n",
    "#         else:\n",
    "#             print(f\"Table {table_name} does not exist. Creating a new one.\")\n",
    "#         df.to_sql(table_name, engine, if_exists=\"replace\", index=False)\n",
    "#         print(f\"Uploaded dataframe to table {table_name}.\")\n",
    "\n",
    "# if DB_PASSWORD:\n",
    "#     DATABASE_URL = f\"{DB_TYPE}://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}{'?driver=' if DB_DRIVER else ''}{DB_DRIVER.replace(' ', '+') if DB_DRIVER else ''}\"\n",
    "\n",
    "#     engine = sqlalchemy.create_engine(DATABASE_URL)\n",
    "\n",
    "#     with engine.connect() as connection:\n",
    "#         print(\"Connection to the database was successful!\")\n",
    "\n",
    "#     upload_dataframes(dfs)\n",
    "# else:\n",
    "#     EXCEL_MAX_ROWS = 1048575\n",
    "#     excel_writer = pd.ExcelWriter(\"../data/dataframes.xlsx\")\n",
    "#     for name, df in dfs.items():\n",
    "#         df.head(EXCEL_MAX_ROWS).to_excel(excel_writer, sheet_name=name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
