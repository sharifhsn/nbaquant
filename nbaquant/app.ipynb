{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA Player Data Analysis Project\n",
    "\n",
    "## Project Outline\n",
    "\n",
    "1. **Introduction**\n",
    "   - Overview of the project\n",
    "   - Goals and objectives\n",
    "\n",
    "2. **Data Collection**\n",
    "   - Import necessary libraries\n",
    "   - Fetch player data from the NBA API\n",
    "\n",
    "3. **Data Processing**\n",
    "   - Load data into a Pandas DataFrame\n",
    "   - Data cleaning and transformation\n",
    "\n",
    "4. **Data Analysis**\n",
    "   - Exploratory data analysis (EDA)\n",
    "   - Visualizations\n",
    "\n",
    "5. **Conclusion**\n",
    "   - Summary of findings\n",
    "   - Future work\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python\n",
    "\n",
    "This project is written in Python, which means that Python must be installed in your environment to run the project. The minimum supported version is 3.10.\n",
    "\n",
    "#### Windows\n",
    "\n",
    "You can use the Windows package manager `winget`, or the [installer](https://www.python.org/downloads/windows/) from the website.\n",
    "```powershell\n",
    "# you can change the version in the package name to your desired version\n",
    "winget install Python.Python.3.12\n",
    "```\n",
    "\n",
    "#### MacOS\n",
    "Python is already installed by default on recent versions of MacOS. If you have an older version that is not supported, you can use the [Homebrew](https://brew.sh/) package manager to install it, or the [installer](https://www.python.org/downloads/macos/) from the website.\n",
    "```zsh\n",
    "brew install python\n",
    "```\n",
    "\n",
    "#### Linux\n",
    "Python is already installed by default on most distributions of Linux. If it isn't, you can use your distribution's package manager to install Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Virtual Environment\n",
    "\n",
    "It's generally recommended that you use a virtual environment (or venv) for this project. That way, all dependencies can be installed for the project without affecting the rest of your system. You can create a venv with Python:\n",
    "\n",
    "```bash\n",
    "python -m venv .venv\n",
    "```\n",
    "\n",
    "To activate the virtual environment in your shell, you can use the following commands.\n",
    "\n",
    "On Windows:\n",
    "\n",
    "```powershell\n",
    ".venv\\Scripts\\activate\n",
    "```\n",
    "\n",
    "On other operating systems:\n",
    "\n",
    "```bash\n",
    ".venv/bin/activate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "\n",
    "This project uses [Poetry](https://python-poetry.org/) to manage its dependencies. You can install the dependencies with the `poetry` command:\n",
    "\n",
    "`poetry install`\n",
    "\n",
    "If you don't want to use Poetry, a `requirements.txt` is also provided. You can install this using `pip`:\n",
    "\n",
    "`pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Variables\n",
    "\n",
    "We will load all our environment variables from a `.env` file, if one is provided.\n",
    "\n",
    "If database information is provided, all dataframes used for analysis are uploaded to it. We use [Microsoft SQL Server](https://www.microsoft.com/en-us/sql-server/sql-server-downloads) by default but any kind of database is supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "DB_TYPE = os.getenv(\"DB_TYPE\", \"sqlserver\")\n",
    "DB_USER = os.getenv(\"DB_USER\", \"sqladmin\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "DB_HOST = os.getenv(\"DB_HOST\", \"localhost\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\", \"1433\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\", \"dataframes\")\n",
    "DB_DRIVER = os.getenv(\"DB_DRIVER\", \"ODBC Driver 18 for SQL Server\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presentation\n",
    "\n",
    "By default, Pandas dataframes are truncated when they are printed. We want to be able to view all of the data at once, so we embed the dataframe in a scrollable element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "def custom_scrollable_display(df: pd.DataFrame, max_height=400):\n",
    "    \"\"\"\n",
    "    Custom display function to render DataFrames as scrollable elements.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: The DataFrame to display.\n",
    "    - max_height: The maximum height of the scrollable area in pixels.\n",
    "    \"\"\"\n",
    "    style = f\"\"\"\n",
    "    <style>\n",
    "    .scrollable-dataframe {{\n",
    "        display: inline-block;\n",
    "        white-space: nowrap;\n",
    "        overflow-x: scroll;\n",
    "        max-height: {max_height}px;\n",
    "        overflow-y: scroll;\n",
    "    }}\n",
    "    </style>\n",
    "    \"\"\"\n",
    "    display(HTML(style + f'<div class=\"scrollable-dataframe\">{df.to_html()}</div>'))\n",
    "\n",
    "def custom_display_hook(df):\n",
    "    custom_scrollable_display(df)\n",
    "    return \"\"\n",
    "\n",
    "# hook up the custom display function to the automatic printer\n",
    "InteractiveShell.instance().display_formatter.formatters['text/html'].for_type(pd.DataFrame, custom_display_hook);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Commit Hooks (Developer Only)\n",
    "\n",
    "This notebook uses `nbstripout` to strip notebook output from Git commits. If you are committing code, please run the following command to set up the Git filter.\n",
    "\n",
    "Poetry is required for the pre-commit hooks, so make sure it is installed before you commit code. You will also need to add the plugin `poetry-plugin-export` in order to run the export hook.\n",
    "```bash\n",
    "poetry self add poetry-plugin-export\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nbstripout --install\n",
    "!pre-commit install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection\n",
    "\n",
    "### Fetch Player Data from NBA API\n",
    "\n",
    "`nba_api` provides static player and team information, which we will download here so that we can reuse it without requesting the API unnecessarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nba_api.stats.static import players, teams\n",
    "PLAYERS_LIST_FILE = \"../data/players_list.json\"\n",
    "TEAMS_LIST_FILE = \"../data/teams_list.json\"\n",
    "\n",
    "if not os.path.exists(PLAYERS_LIST_FILE):\n",
    "    players_list = players.get_players()\n",
    "    with open(PLAYERS_LIST_FILE, \"w\") as f:\n",
    "        json.dump(players_list, f)\n",
    "else:\n",
    "    with open(PLAYERS_LIST_FILE, \"r\") as f:\n",
    "        players_list = json.load(f)\n",
    "if not os.path.exists(TEAMS_LIST_FILE):\n",
    "    teams_list = teams.get_teams()\n",
    "    teams.get_wnba_teams\n",
    "    with open(TEAMS_LIST_FILE, \"w\") as f:\n",
    "        json.dump(teams_list, f)\n",
    "else:\n",
    "    with open(TEAMS_LIST_FILE, \"r\") as f:\n",
    "        teams_list = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch Game Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're only interested in games that are either in the regular season or in the playoffs. We'll add an enum to distinguish the type of game and use it to differentiate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class SeasonType(Enum):\n",
    "    PRESEASON = 1\n",
    "    REGULAR_SEASON = 2\n",
    "    ALL_STAR = 3\n",
    "    PLAYOFFS = 4\n",
    "    PLAY_IN = 5\n",
    "    NBA_CUP = 6\n",
    "class Season():\n",
    "    def __init__(self, season_id: int) -> None:\n",
    "        season_id_str = str(season_id)\n",
    "        self.season_type = SeasonType(int(season_id_str[0]))\n",
    "        self.season_year = int(season_id_str[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nba_api.stats.endpoints import leaguegamefinder\n",
    "from nba_api.stats.library.parameters import LeagueIDNullable, LeagueID\n",
    "START_SEASON = 2012\n",
    "END_SEASON = 2023\n",
    "GAMES_LIST_FILE = \"../data/games_list.csv\"\n",
    "if os.path.exists(GAMES_LIST_FILE):\n",
    "    games_list: pd.DataFrame = pd.read_csv(GAMES_LIST_FILE)\n",
    "else:\n",
    "    games_list = pd.DataFrame()\n",
    "    for season in range(START_SEASON, END_SEASON + 1):\n",
    "        # put season into the correct form e.g. 2023 -> 2023-24\n",
    "        season_str = f\"{season}-{str(season + 1)[2:]}\"\n",
    "        print(f\"Fetching games for season: {season_str}\", end=\"\\r\")\n",
    "        gamefinder = leaguegamefinder.LeagueGameFinder(season_nullable=season_str, league_id_nullable=LeagueIDNullable.nba)\n",
    "        games = gamefinder.get_data_frames()[0]\n",
    "        games_list = pd.concat([games_list, games], ignore_index=True)\n",
    "        time.sleep(0.6)\n",
    "    games_list.to_csv(GAMES_LIST_FILE, index=False)\n",
    "# games_list[\"SEASON_ID\"].unique()\n",
    "games_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch Play by Plays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nba_api.stats.endpoints import playbyplayv3\n",
    "from requests.exceptions import ReadTimeout\n",
    "# took 483 minutes to download up to 2012\n",
    "PBP_LIST_FILE = \"../data/pbp_list.csv\"\n",
    "if os.path.exists(PBP_LIST_FILE):\n",
    "    pbp_list = pd.read_csv(PBP_LIST_FILE)\n",
    "else:\n",
    "    unique_games_list = games_list.drop_duplicates(subset=\"GAME_ID\")\n",
    "    pbp_list = pd.DataFrame()\n",
    "    for index, row in unique_games_list.iterrows():\n",
    "        err = False\n",
    "        game_id = row[\"GAME_ID\"]\n",
    "        game_date = row[\"GAME_DATE\"]\n",
    "        season_id = row[\"SEASON_ID\"]\n",
    "        season = Season(season_id)\n",
    "        if season.season_type != SeasonType.REGULAR_SEASON and season.season_type != SeasonType.PLAYOFFS:\n",
    "            continue\n",
    "        print(f\"Fetching play by play for game {game_id} on {game_date}\", end=\"\\r\")\n",
    "        while True:\n",
    "            try:\n",
    "                pbpfinder = playbyplayv3.PlayByPlayV3(f\"{game_id:010}\")\n",
    "                break\n",
    "            except ReadTimeout as e:\n",
    "                print(f\"{e}! Try again\")\n",
    "            except Exception:\n",
    "                with open(\"../data/err.log\", \"a\") as f:\n",
    "                    print(f\"{game_id} does not have a play by play\", file=f)\n",
    "                err = True\n",
    "                break\n",
    "        if err:\n",
    "            continue\n",
    "        pbp = pbpfinder.get_data_frames()[0]\n",
    "        pbp_list = pd.concat([pbp_list, pbp], ignore_index=True)\n",
    "        time.sleep(0.6)\n",
    "    pbp_list.to_csv(PBP_LIST_FILE, index=False)\n",
    "pbp_list.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a class to store the clock time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clock():\n",
    "    def __init__(self, clock) -> None:\n",
    "        minutes = int(clock[2:4])\n",
    "        seconds = int(clock[5:7])\n",
    "        self.total_seconds = minutes * 60 + seconds\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        minutes, seconds = divmod(self.total_seconds, 60)\n",
    "        return f\"{minutes:02}:{int(seconds):02}\"\n",
    "\n",
    "    def __hash__(self) -> int:\n",
    "        return hash(self.total_seconds)\n",
    "\n",
    "    def __int__(self):\n",
    "        return self.total_seconds\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, Clock):\n",
    "            return self.total_seconds == other.total_seconds\n",
    "        return self.total_seconds == other\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        if isinstance(other, Clock):\n",
    "            return self.total_seconds < other.total_seconds\n",
    "        return self.total_seconds < other\n",
    "\n",
    "    def __le__(self, other):\n",
    "        return self < other or self == other\n",
    "\n",
    "    def __gt__(self, other):\n",
    "        return not self <= other\n",
    "\n",
    "    def __ge__(self, other):\n",
    "        return not self < other\n",
    "\n",
    "    def __add__(self, other):\n",
    "        if isinstance(other, Clock):\n",
    "            return Clock(self.total_seconds + other.total_seconds)\n",
    "        return Clock(self.total_seconds + other)\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        if isinstance(other, Clock):\n",
    "            return Clock(self.total_seconds - other.total_seconds)\n",
    "        return Clock(self.total_seconds - other)\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        return Clock(other - self.total_seconds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and transform the clock time to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_clock(clock):\n",
    "    if isinstance(clock, Clock):\n",
    "        return clock\n",
    "    else:\n",
    "        return Clock(clock)\n",
    "pbp_list[\"clock\"] = pbp_list[\"clock\"].apply(to_clock)\n",
    "pbp_list[\"clock\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our analysis will be focused on when shots with assists are being made, so we will isolate this dataframe for the relevant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "made_shots = pbp_list.loc[pbp_list[\"actionType\"] == \"Made Shot\"].reset_index(drop=True)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "df[\"clock\"] = made_shots[\"clock\"]\n",
    "\n",
    "# we're not using the number numerically, so we can mark this as a categorical variable to save on memory and storage\n",
    "df[\"period\"] = pd.Categorical(made_shots[\"period\"])\n",
    "\n",
    "df[\"has_assist\"] = made_shots[\"description\"].str.contains(\"AST\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Reporting\n",
    "\n",
    "To create our final report from our analysis, we will be using Power BI. We have a Microsoft SQL Server database that our Power BI report will import the tables from. If no database is available, the dataframes will instead export as an Excel spreadsheet, which can be manually uploaded to Power BI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine import reflection\n",
    "DATABASE_URL = f\"{DB_TYPE}://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}{'?driver=' if DB_DRIVER else ''}{DB_DRIVER.replace(' ', '+') if DB_DRIVER else ''}\"\n",
    "\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    display(\"Connection to the database was successful!\")\n",
    "\n",
    "def upload_dataframes(dfs: Dict[str, pd.DataFrame]) -> None:\n",
    "    inspector = reflection.Inspector.from_engine(engine)\n",
    "    existing_tables = inspector.get_table_names()\n",
    "\n",
    "    for table_name, df in dfs.items():\n",
    "        # check if table already exists\n",
    "        if table_name in existing_tables:\n",
    "            # if there are no changes to the table, do not write to it\n",
    "            existing_df = pd.read_sql_table(table_name, engine)\n",
    "            if df.shape == existing_df.shape and df.equals(existing_df):\n",
    "                print(f\"No changes detected for table {table_name}. Skipping upload.\")\n",
    "                continue\n",
    "        else:\n",
    "            print(f\"Table {table_name} does not exist. Creating a new one.\")\n",
    "        df.to_sql(table_name, engine, if_exists=\"replace\", index=False)\n",
    "        print(f\"Uploaded dataframe to table {table_name}.\")\n",
    "\n",
    "upload_dataframes({\"assists_in_the_clutch\": df})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
